{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5eb403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the cat join a band?\\n\\nBecause it wanted to be a purr-cussionist!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 41, 'total_tokens': 62, 'completion_time': 0.051123191, 'prompt_time': 0.002175733, 'queue_time': 0.015528084000000001, 'total_time': 0.053298924}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--842a6350-fcf3-46dd-a50b-def984fe8b2b-0', usage_metadata={'input_tokens': 41, 'output_tokens': 21, 'total_tokens': 62})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "import os \n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "# result = model.invoke(\"Genrate a joke on cats\")\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef24f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACE_API_KEY'] = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c73ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_groq import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2dd7a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\n",
    "    \"https://python.langchain.com/docs/introduction/\",\n",
    "    \"https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html\",\n",
    "    \"https://python.langchain.com/api_reference/core/agents.html\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4374df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ðŸ¦œï¸ðŸ”— LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nIntroduction | ðŸ¦œï¸ðŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ðŸ’¬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?EcosystemðŸ¦œðŸ› ï¸ LangSmithðŸ¦œðŸ•¸ï¸ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyIntroductionOn this pageIntroduction\\nLangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain\\'s open-source components and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\\n\\n\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the integrations page for\\nmore.\\n\\nSelect chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nmodel.invoke(\"Hello, world!\")\\nnoteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nArchitecture\\u200b\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\nArchitecture page.\\n\\nlangchain-core: Base abstractions for chat models and other components.\\nIntegration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\nlangchain: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\nlangchain-community: Third-party integrations that are community maintained.\\nlanggraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\\n\\nGuides\\u200b\\nTutorials\\u200b\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our tutorials section.\\nThis is the best place to get started.\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\nBuild a Chatbot\\nBuild an Agent\\nIntroduction to LangGraph\\n\\nExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\nHow-to guides\\u200b\\nHere youâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the Tutorials and the API Reference.\\nHowever, these guides will help you quickly accomplish common tasks using chat models,\\nvector stores, and other common LangChain components.\\nCheck out LangGraph-specific how-tos here.\\nConceptual guide\\u200b\\nIntroductions to all the key parts of LangChain youâ€™ll need to know! Here you\\'ll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our growing list of integrations.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\nðŸ¦œðŸ› ï¸ LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\nðŸ¦œðŸ•¸ï¸ LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\nSecurity\\u200b\\nRead up on security best practices to make sure you\\'re developing safely with LangChain.\\nContributing\\u200b\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.Edit this pageNextTutorialsArchitectureGuidesTutorialsHow-to guidesConceptual guideIntegrationsAPI referenceEcosystemðŸ¦œðŸ› ï¸ LangSmithðŸ¦œðŸ•¸ï¸ LangGraphAdditional resourcesVersionsSecurityContributingCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html', 'title': 'Runnable â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\nRunnable â€” ðŸ¦œðŸ”— LangChain  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\nBack to top\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages\\n\\nCore\\nagents\\nbeta\\ncaches\\ncallbacks\\nchat_history\\nchat_loaders\\nchat_sessions\\ndocument_loaders\\ndocuments\\nembeddings\\nexample_selectors\\nexceptions\\nglobals\\nindexing\\nlanguage_models\\nload\\nmessages\\noutput_parsers\\noutputs\\nprompt_values\\nprompts\\nrate_limiters\\nretrievers\\nrunnables\\nRunnable\\nRunnableBinding\\nRunnableBindingBase\\nRunnableEach\\nRunnableEachBase\\nRunnableGenerator\\nRunnableLambda\\nRunnableMap\\nRunnableParallel\\nRunnableSequence\\nRunnableSerializable\\nRunnableBranch\\nContextThreadPoolExecutor\\nEmptyDict\\nRunnableConfig\\nDynamicRunnable\\nRunnableConfigurableAlternatives\\nRunnableConfigurableFields\\nStrEnum\\nRunnableWithFallbacks\\nBranch\\nCurveStyle\\nEdge\\nGraph\\nLabelsDict\\nMermaidDrawMethod\\nNode\\nNodeStyles\\nStringifiable\\nAsciiCanvas\\nVertexViewer\\nPngDrawer\\nRunnableWithMessageHistory\\nRunnableAssign\\nRunnablePassthrough\\nRunnablePick\\nExponentialJitterParams\\nRunnableRetry\\nRouterInput\\nRouterRunnable\\nBaseStreamEvent\\nCustomStreamEvent\\nEventData\\nStandardStreamEvent\\nAddableDict\\nConfigurableField\\nConfigurableFieldMultiOption\\nConfigurableFieldSingleOption\\nConfigurableFieldSpec\\nFunctionNonLocals\\nGetLambdaSource\\nIsFunctionArgDict\\nIsLocalDict\\nNonLocals\\nSupportsAdd\\nchain\\ncoerce_to_runnable\\nacall_func_with_variable_args\\ncall_func_with_variable_args\\nensure_config\\nget_async_callback_manager_for_config\\nget_callback_manager_for_config\\nget_config_list\\nget_executor_for_config\\nmerge_configs\\npatch_config\\nrun_in_executor\\nset_config_context\\nmake_options_spec\\nprefix_config_spec\\nis_uuid\\nnode_data_json\\nnode_data_str\\ndraw_ascii\\ndraw_mermaid\\ndraw_mermaid_png\\naidentity\\nidentity\\naadd\\naccepts_config\\naccepts_context\\naccepts_run_manager\\nadd\\ncoro_with_context\\ngated_coro\\ngather_with_concurrency\\nget_function_first_arg_dict_keys\\nget_lambda_source\\nget_unique_config_specs\\nindent_lines_after_first\\nis_async_callable\\nis_async_generator\\n\\n\\nstores\\nstructured_query\\nsys_info\\ntools\\ntracers\\nutils\\nvectorstores\\n\\n\\nLangchain\\nText Splitters\\nCommunity\\nExperimental\\n\\nIntegrations\\n\\nAnthropic\\nAstraDB\\nAWS\\nAzure Ai\\nAzure Dynamic Sessions\\nCerebras\\nChroma\\nCli\\nCohere\\nDb2\\nDeepseek\\nElasticsearch\\nExa\\nFireworks\\nGoogle Community\\nGoogle GenAI\\nGoogle VertexAI\\nGroq\\nHuggingface\\nIBM\\nLangchain_V1\\nMilvus\\nMistralAI\\nMongoDB\\nNeo4J\\nNomic\\nNvidia Ai Endpoints\\nOllama\\nOpenAI\\nPerplexity\\nPinecone\\nPostgres\\nPrompty\\nQdrant\\nRedis\\nSema4\\nSnowflake\\nSqlserver\\nStandard Tests\\nTavily\\nTogether\\nUnstructured\\nUpstage\\nWeaviate\\nXAI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Python API Reference\\nlangchain-core: 0.3.72\\nrunnables\\nRunnable\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRunnable#\\n\\n\\nclass langchain_core.runnables.base.Runnable[source]#\\nA unit of work that can be invoked, batched, streamed, transformed and composed.\\n\\nKey Methods#\\n\\ninvoke/ainvoke: Transforms a single input into an output.\\nbatch/abatch: Efficiently transforms multiple inputs into outputs.\\nstream/astream: Streams output from a single input as itâ€™s produced.\\nastream_log: Streams output and selected intermediate results from an input.\\n\\nBuilt-in optimizations:\\n\\nBatch: By default, batch runs invoke() in parallel using a thread pool executor.\\nOverride to optimize batching.\\nAsync: Methods with â€œaâ€ suffix are asynchronous. By default, they execute\\nthe sync counterpart using asyncioâ€™s thread pool.\\nOverride for native async.\\n\\nAll methods accept an optional config argument, which can be used to configure\\nexecution, add tags and metadata for tracing and debugging etc.\\nRunnables expose schematic information about their input, output and config via\\nthe input_schema property, the output_schema property and config_schema method.\\n\\n\\nLCEL and Composition#\\nThe LangChain Expression Language (LCEL) is a declarative way to compose Runnables\\ninto chains. Any chain constructed this way will automatically have sync, async,\\nbatch, and streaming support.\\nThe main composition primitives are RunnableSequence and RunnableParallel.\\nRunnableSequence invokes a series of runnables sequentially, with\\none Runnableâ€™s output serving as the nextâ€™s input. Construct using\\nthe | operator or by passing a list of runnables to RunnableSequence.\\nRunnableParallel invokes runnables concurrently, providing the same input\\nto each. Construct it using a dict literal within a sequence or by passing a\\ndict to RunnableParallel.\\nFor example,\\nfrom langchain_core.runnables import RunnableLambda\\n\\n# A RunnableSequence constructed using the `|` operator\\nsequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\\nsequence.invoke(1) # 4\\nsequence.batch([1, 2, 3]) # [4, 6, 8]\\n\\n\\n# A sequence that contains a RunnableParallel constructed using a dict literal\\nsequence = RunnableLambda(lambda x: x + 1) | {\\n    \\'mul_2\\': RunnableLambda(lambda x: x * 2),\\n    \\'mul_5\\': RunnableLambda(lambda x: x * 5)\\n}\\nsequence.invoke(1) # {\\'mul_2\\': 4, \\'mul_5\\': 10}\\n\\n\\n\\n\\nStandard Methods#\\nAll Runnables expose additional methods that can be used to modify their behavior\\n(e.g., add a retry policy, add lifecycle listeners, make them configurable, etc.).\\nThese methods will work on any Runnable, including Runnable chains constructed\\nby composing other Runnables. See the individual methods for details.\\nFor example,\\nfrom langchain_core.runnables import RunnableLambda\\n\\nimport random\\n\\ndef add_one(x: int) -> int:\\n    return x + 1\\n\\n\\ndef buggy_double(y: int) -> int:\\n    \"\"\"Buggy code that will fail 70% of the time\"\"\"\\n    if random.random() > 0.3:\\n        print(\\'This code failed, and will probably be retried!\\')  # noqa: T201\\n        raise ValueError(\\'Triggered buggy code\\')\\n    return y * 2\\n\\nsequence = (\\n    RunnableLambda(add_one) |\\n    RunnableLambda(buggy_double).with_retry( # Retry on failure\\n        stop_after_attempt=10,\\n        wait_exponential_jitter=False\\n    )\\n)\\n\\nprint(sequence.input_schema.model_json_schema()) # Show inferred input schema\\nprint(sequence.output_schema.model_json_schema()) # Show inferred output schema\\nprint(sequence.invoke(2)) # invoke the sequence (note the retry above!!)\\n\\n\\n\\n\\nDebugging and tracing#\\nAs the chains get longer, it can be useful to be able to see intermediate results\\nto debug and trace the chain.\\nYou can set the global debug flag to True to enable debug output for all chains:\\n\\nfrom langchain_core.globals import set_debug\\nset_debug(True)\\n\\n\\n\\nAlternatively, you can pass existing or custom callbacks to any given chain:\\n\\nfrom langchain_core.tracers import ConsoleCallbackHandler\\n\\nchain.invoke(\\n    ...,\\n    config={\\'callbacks\\': [ConsoleCallbackHandler()]}\\n)\\n\\n\\n\\nFor a UI (and much more) checkout LangSmith: https://docs.smith.langchain.com/\\nAttributes\\n\\n\\nInputType\\nThe type of input this Runnable accepts specified as a type annotation.\\n\\nOutputType\\nThe type of output this Runnable produces specified as a type annotation.\\n\\nconfig_specs\\nList configurable fields for this Runnable.\\n\\ninput_schema\\nThe type of input this Runnable accepts specified as a pydantic model.\\n\\noutput_schema\\nThe type of output this Runnable produces specified as a pydantic model.\\n\\n\\n\\n\\nMethods\\n\\n\\nabatch(inputs[,\\xa0config,\\xa0return_exceptions])\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\n\\nabatch_as_completed()\\nRun ainvoke in parallel on a list of inputs.\\n\\nainvoke(input[,\\xa0config])\\nDefault implementation of ainvoke, calls invoke from a thread.\\n\\nas_tool([args_schema,\\xa0name,\\xa0description,\\xa0...])\\n\\n\\nassign(**kwargs)\\nAssigns new fields to the dict output of this Runnable.\\n\\nastream(input[,\\xa0config])\\nDefault implementation of astream, which calls ainvoke.\\n\\nastream_events(input[,\\xa0config,\\xa0version,\\xa0...])\\nGenerate a stream of events.\\n\\nastream_log()\\nStream all output from a Runnable, as reported to the callback system.\\n\\natransform(input[,\\xa0config])\\nDefault implementation of atransform, which buffers input and calls astream.\\n\\nbatch(inputs[,\\xa0config,\\xa0return_exceptions])\\nDefault implementation runs invoke in parallel using a thread pool executor.\\n\\nbatch_as_completed()\\nRun invoke in parallel on a list of inputs.\\n\\nbind(**kwargs)\\nBind arguments to a Runnable, returning a new Runnable.\\n\\nconfig_schema(*[,\\xa0include])\\nThe type of config this Runnable accepts specified as a pydantic model.\\n\\nget_config_jsonschema(*[,\\xa0include])\\nGet a JSON schema that represents the config of the Runnable.\\n\\nget_graph([config])\\nReturn a graph representation of this Runnable.\\n\\nget_input_jsonschema([config])\\nGet a JSON schema that represents the input to the Runnable.\\n\\nget_input_schema([config])\\nGet a pydantic model that can be used to validate input to the Runnable.\\n\\nget_name([suffix,\\xa0name])\\nGet the name of the Runnable.\\n\\nget_output_jsonschema([config])\\nGet a JSON schema that represents the output of the Runnable.\\n\\nget_output_schema([config])\\nGet a pydantic model that can be used to validate output to the Runnable.\\n\\nget_prompts([config])\\nReturn a list of prompts used by this Runnable.\\n\\ninvoke(input[,\\xa0config])\\nTransform a single input into an output.\\n\\nmap()\\nReturn a new Runnable that maps a list of inputs to a list of outputs.\\n\\npick(keys)\\nPick keys from the output dict of this Runnable.\\n\\npipe(*others[,\\xa0name])\\nCompose this Runnable with Runnable-like objects to make a RunnableSequence.\\n\\nstream(input[,\\xa0config])\\nDefault implementation of stream, which calls invoke.\\n\\ntransform(input[,\\xa0config])\\nDefault implementation of transform, which buffers input and calls astream.\\n\\nwith_alisteners(*[,\\xa0on_start,\\xa0on_end,\\xa0on_error])\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\n\\nwith_config([config])\\nBind config to a Runnable, returning a new Runnable.\\n\\nwith_fallbacks(fallbacks,\\xa0*[,\\xa0...])\\nAdd fallbacks to a Runnable, returning a new Runnable.\\n\\nwith_listeners(*[,\\xa0on_start,\\xa0on_end,\\xa0on_error])\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\n\\nwith_retry(*[,\\xa0retry_if_exception_type,\\xa0...])\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nwith_types(*[,\\xa0input_type,\\xa0output_type])\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\n\\n\\n\\n\\n\\nasync abatch(\\n\\ninputs: list[Input],\\nconfig: RunnableConfig | list[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) â†’ list[Output][source]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input]) â€“ A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like \\'tags\\', \\'metadata\\' for\\ntracing purposes, \\'max_concurrency\\' for controlling how much work to\\ndo in parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) â€“ Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: Literal[False] = False,\\n**kwargs: Any | None,\\n\\n) â†’ AsyncIterator[tuple[int, Output]][source]#\\n\\nasync abatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: Literal[True],\\n**kwargs: Any | None,\\n\\n) â†’ AsyncIterator[tuple[int, Output | Exception]]\\nRun ainvoke in parallel on a list of inputs.\\nYields results as they complete.\\n\\nParameters:\\n\\ninputs â€“ A list of inputs to the Runnable.\\nconfig â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like \\'tags\\', \\'metadata\\' for\\ntracing purposes, \\'max_concurrency\\' for controlling how much work to\\ndo in parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions â€“ Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\n\\n\\n\\n\\nasync ainvoke(\\n\\ninput: Input,\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any,\\n\\n) â†’ Output[source]#\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe Runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters:\\n\\ninput (Input)\\nconfig (RunnableConfig | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nOutput\\n\\n\\n\\n\\n\\nas_tool(\\n\\nargs_schema: type[BaseModel] | None = None,\\n*,\\nname: str | None = None,\\ndescription: str | None = None,\\narg_types: dict[str, type] | None = None,\\n\\n) â†’ BaseTool[source]#\\n\\nBeta\\nThis API is in beta and may change in the future.\\n\\nCreate a BaseTool from a Runnable.\\nas_tool will instantiate a BaseTool with a name, description, and\\nargs_schema from a Runnable. Where possible, schemas are inferred\\nfrom runnable.get_input_schema. Alternatively (e.g., if the\\nRunnable takes a dict as input and the specific dict keys are not typed),\\nthe schema can be specified directly with args_schema. You can also\\npass arg_types to just specify the required arguments and their types.\\n\\nParameters:\\n\\nargs_schema (Optional[type[BaseModel]]) â€“ The schema for the tool. Defaults to None.\\nname (Optional[str]) â€“ The name of the tool. Defaults to None.\\ndescription (Optional[str]) â€“ The description of the tool. Defaults to None.\\narg_types (Optional[dict[str, type]]) â€“ A dictionary of argument names to types. Defaults to None.\\n\\n\\nReturns:\\nA BaseTool instance.\\n\\nReturn type:\\nBaseTool\\n\\n\\nTyped dict input:\\nfrom typing_extensions import TypedDict\\nfrom langchain_core.runnables import RunnableLambda\\n\\nclass Args(TypedDict):\\n    a: int\\n    b: list[int]\\n\\ndef f(x: Args) -> str:\\n    return str(x[\"a\"] * max(x[\"b\"]))\\n\\nrunnable = RunnableLambda(f)\\nas_tool = runnable.as_tool()\\nas_tool.invoke({\"a\": 3, \"b\": [1, 2]})\\n\\n\\ndict input, specifying schema via args_schema:\\nfrom typing import Any\\nfrom pydantic import BaseModel, Field\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef f(x: dict[str, Any]) -> str:\\n    return str(x[\"a\"] * max(x[\"b\"]))\\n\\nclass FSchema(BaseModel):\\n    \"\"\"Apply a function to an integer and list of integers.\"\"\"\\n\\n    a: int = Field(..., description=\"Integer\")\\n    b: list[int] = Field(..., description=\"List of ints\")\\n\\nrunnable = RunnableLambda(f)\\nas_tool = runnable.as_tool(FSchema)\\nas_tool.invoke({\"a\": 3, \"b\": [1, 2]})\\n\\n\\ndict input, specifying schema via arg_types:\\nfrom typing import Any\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef f(x: dict[str, Any]) -> str:\\n    return str(x[\"a\"] * max(x[\"b\"]))\\n\\nrunnable = RunnableLambda(f)\\nas_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\\nas_tool.invoke({\"a\": 3, \"b\": [1, 2]})\\n\\n\\nString input:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef f(x: str) -> str:\\n    return x + \"a\"\\n\\ndef g(x: str) -> str:\\n    return x + \"z\"\\n\\nrunnable = RunnableLambda(f) | g\\nas_tool = runnable.as_tool()\\nas_tool.invoke(\"b\")\\n\\n\\n\\nAdded in version 0.2.14.\\n\\n\\n\\n\\nassign(\\n\\n**kwargs: Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any] | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]],\\n\\n) â†’ RunnableSerializable[Any, Any][source]#\\nAssigns new fields to the dict output of this Runnable.\\nReturns a new Runnable.\\nfrom langchain_community.llms.fake import FakeStreamingListLLM\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import SystemMessagePromptTemplate\\nfrom langchain_core.runnables import Runnable\\nfrom operator import itemgetter\\n\\nprompt = (\\n    SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\\n    + \"{question}\"\\n)\\nllm = FakeStreamingListLLM(responses=[\"foo-lish\"])\\n\\nchain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\\n\\nchain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\\n\\nprint(chain_with_assign.input_schema.model_json_schema())\\n# {\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\':\\n{\\'question\\': {\\'title\\': \\'Question\\', \\'type\\': \\'string\\'}}}\\nprint(chain_with_assign.output_schema.model_json_schema())\\n# {\\'title\\': \\'RunnableSequenceOutput\\', \\'type\\': \\'object\\', \\'properties\\':\\n{\\'str\\': {\\'title\\': \\'Str\\',\\n\\'type\\': \\'string\\'}, \\'hello\\': {\\'title\\': \\'Hello\\', \\'type\\': \\'string\\'}}}\\n\\n\\n\\nParameters:\\nkwargs (Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any] | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]])\\n\\nReturn type:\\nRunnableSerializable[Any, Any]\\n\\n\\n\\n\\n\\nasync astream(\\n\\ninput: Input,\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any | None,\\n\\n) â†’ AsyncIterator[Output][source]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]\\n\\n\\n\\n\\n\\nasync astream_events(\\n\\ninput: Any,\\nconfig: RunnableConfig | None = None,\\n*,\\nversion: Literal[\\'v1\\', \\'v2\\'] = \\'v2\\',\\ninclude_names: Sequence[str] | None = None,\\ninclude_types: Sequence[str] | None = None,\\ninclude_tags: Sequence[str] | None = None,\\nexclude_names: Sequence[str] | None = None,\\nexclude_types: Sequence[str] | None = None,\\nexclude_tags: Sequence[str] | None = None,\\n**kwargs: Any,\\n\\n) â†’ AsyncIterator[StreamEvent][source]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\nevent: str - Event names are of the format:\\non_[runnable_type]_(start|stream|end).\\nname: str - The name of the Runnable that generated the event.\\nrun_id: str - randomly generated ID associated with the given\\nexecution of the Runnable that emitted the event. A child Runnable that gets\\ninvoked as part of the execution of a parent Runnable is assigned its own\\nunique ID.\\nparent_ids: list[str] - The IDs of the parent runnables that generated\\nthe event. The root Runnable will have an empty list. The order of the parent\\nIDs is from the root to the immediate parent. Only available for v2 version of\\nthe API. The v1 version of the API will return an empty list.\\ntags: Optional[list[str]] - The tags of the Runnable that generated\\nthe event.\\nmetadata: Optional[dict[str, Any]] - The metadata of the Runnable that\\ngenerated the event.\\ndata: dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\n\\nNote\\nThis reference table is for the V2 version of the schema.\\n\\n\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\n\\n\\non_chat_model_start\\n[model name]\\n\\n{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}\\n\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=â€helloâ€)\\n\\n\\n\\non_chat_model_end\\n[model name]\\n\\n{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=â€hello worldâ€)\\n\\non_llm_start\\n[model name]\\n\\n{â€˜inputâ€™: â€˜helloâ€™}\\n\\n\\non_llm_stream\\n[model name]\\nâ€˜Helloâ€™\\n\\n\\n\\non_llm_end\\n[model name]\\n\\nâ€˜Hello human!â€™\\n\\n\\non_chain_start\\nformat_docs\\n\\n\\n\\n\\non_chain_stream\\nformat_docs\\nâ€œhello world!, goodbye world!â€\\n\\n\\n\\non_chain_end\\nformat_docs\\n\\n[Document(â€¦)]\\nâ€œhello world!, goodbye world!â€\\n\\non_tool_start\\nsome_tool\\n\\n{â€œxâ€: 1, â€œyâ€: â€œ2â€}\\n\\n\\non_tool_end\\nsome_tool\\n\\n\\n{â€œxâ€: 1, â€œyâ€: â€œ2â€}\\n\\non_retriever_start\\n[retriever name]\\n\\n{â€œqueryâ€: â€œhelloâ€}\\n\\n\\non_retriever_end\\n[retriever name]\\n\\n{â€œqueryâ€: â€œhelloâ€}\\n[Document(â€¦), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{â€œquestionâ€: â€œhelloâ€}\\n\\n\\non_prompt_end\\n[template_name]\\n\\n{â€œquestionâ€: â€œhelloâ€}\\nChatPromptValue(messages: [SystemMessage, â€¦])\\n\\n\\n\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\n\\nAttribute\\nType\\nDescription\\n\\n\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\n\\n\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: list[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\n\\n\\nParameters:\\n\\ninput (Any) â€“ The input to the Runnable.\\nconfig (Optional[RunnableConfig]) â€“ The config to use for the Runnable.\\nversion (Literal[\\'v1\\', \\'v2\\']) â€“ The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Optional[Sequence[str]]) â€“ Only include events from runnables with matching names.\\ninclude_types (Optional[Sequence[str]]) â€“ Only include events from runnables with matching types.\\ninclude_tags (Optional[Sequence[str]]) â€“ Only include events from runnables with matching tags.\\nexclude_names (Optional[Sequence[str]]) â€“ Exclude events from runnables with matching names.\\nexclude_types (Optional[Sequence[str]]) â€“ Exclude events from runnables with matching types.\\nexclude_tags (Optional[Sequence[str]]) â€“ Exclude events from runnables with matching tags.\\nkwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.\\n\\n\\nYields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError â€“ If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StreamEvent]\\n\\n\\n\\n\\n\\nasync astream_log(\\n\\ninput: Any,\\nconfig: RunnableConfig | None = None,\\n*,\\ndiff: Literal[True] = True,\\nwith_streamed_output_list: bool = True,\\ninclude_names: Sequence[str] | None = None,\\ninclude_types: Sequence[str] | None = None,\\ninclude_tags: Sequence[str] | None = None,\\nexclude_names: Sequence[str] | None = None,\\nexclude_types: Sequence[str] | None = None,\\nexclude_tags: Sequence[str] | None = None,\\n**kwargs: Any,\\n\\n) â†’ AsyncIterator[RunLogPatch][source]#\\n\\nasync astream_log(\\n\\ninput: Any,\\nconfig: RunnableConfig | None = None,\\n*,\\ndiff: Literal[False],\\nwith_streamed_output_list: bool = True,\\ninclude_names: Sequence[str] | None = None,\\ninclude_types: Sequence[str] | None = None,\\ninclude_tags: Sequence[str] | None = None,\\nexclude_names: Sequence[str] | None = None,\\nexclude_types: Sequence[str] | None = None,\\nexclude_tags: Sequence[str] | None = None,\\n**kwargs: Any,\\n\\n) â†’ AsyncIterator[RunLog]\\nStream all output from a Runnable, as reported to the callback system.\\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\\nOutput is streamed as Log objects, which include a list of\\nJsonpatch ops that describe how the state of the run has changed in each\\nstep, and the final state of the run.\\nThe Jsonpatch ops can be applied in order to construct state.\\n\\nParameters:\\n\\ninput â€“ The input to the Runnable.\\nconfig â€“ The config to use for the Runnable.\\ndiff â€“ Whether to yield diffs between each step or the current state.\\nwith_streamed_output_list â€“ Whether to yield the streamed_output list.\\ninclude_names â€“ Only include logs with these names.\\ninclude_types â€“ Only include logs with these types.\\ninclude_tags â€“ Only include logs with these tags.\\nexclude_names â€“ Exclude logs with these names.\\nexclude_types â€“ Exclude logs with these types.\\nexclude_tags â€“ Exclude logs with these tags.\\nkwargs â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nA RunLogPatch or RunLog object.\\n\\n\\n\\n\\n\\nasync atransform(\\n\\ninput: AsyncIterator[Input],\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any | None,\\n\\n) â†’ AsyncIterator[Output][source]#\\nDefault implementation of atransform, which buffers input and calls astream.\\nSubclasses should override this method if they can start producing output while\\ninput is still being generated.\\n\\nParameters:\\n\\ninput (AsyncIterator[Input]) â€“ An async iterator of inputs to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]\\n\\n\\n\\n\\n\\nbatch(\\n\\ninputs: list[Input],\\nconfig: RunnableConfig | list[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) â†’ list[Output][source]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nbatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: Literal[False] = False,\\n**kwargs: Any,\\n\\n) â†’ Iterator[tuple[int, Output]][source]#\\n\\nbatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: Literal[True],\\n**kwargs: Any,\\n\\n) â†’ Iterator[tuple[int, Output | Exception]]\\nRun invoke in parallel on a list of inputs.\\nYields results as they complete.\\n\\n\\n\\nbind(\\n\\n**kwargs: Any,\\n\\n) â†’ Runnable[Input, Output][source]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) â€“ The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_ollama import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\n\\n\\n\\n\\nconfig_schema(\\n\\n*,\\ninclude: Sequence[str] | None = None,\\n\\n) â†’ type[BaseModel][source]#\\nThe type of config this Runnable accepts specified as a pydantic model.\\nTo mark a field as configurable, see the configurable_fields\\nand configurable_alternatives methods.\\n\\nParameters:\\ninclude (Sequence[str] | None) â€“ A list of fields to include in the config schema.\\n\\nReturns:\\nA pydantic model that can be used to validate config.\\n\\nReturn type:\\ntype[BaseModel]\\n\\n\\n\\n\\n\\nget_config_jsonschema(\\n\\n*,\\ninclude: Sequence[str] | None = None,\\n\\n) â†’ dict[str, Any][source]#\\nGet a JSON schema that represents the config of the Runnable.\\n\\nParameters:\\ninclude (Sequence[str] | None) â€“ A list of fields to include in the config schema.\\n\\nReturns:\\nA JSON schema that represents the config of the Runnable.\\n\\nReturn type:\\ndict[str, Any]\\n\\n\\n\\nAdded in version 0.3.0.\\n\\n\\n\\n\\nget_graph(\\n\\nconfig: RunnableConfig | None = None,\\n\\n) â†’ Graph[source]#\\nReturn a graph representation of this Runnable.\\n\\nParameters:\\nconfig (RunnableConfig | None)\\n\\nReturn type:\\nGraph\\n\\n\\n\\n\\n\\nget_input_jsonschema(\\n\\nconfig: RunnableConfig | None = None,\\n\\n) â†’ dict[str, Any][source]#\\nGet a JSON schema that represents the input to the Runnable.\\n\\nParameters:\\nconfig (RunnableConfig | None) â€“ A config to use when generating the schema.\\n\\nReturns:\\nA JSON schema that represents the input to the Runnable.\\n\\nReturn type:\\ndict[str, Any]\\n\\n\\nExample\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef add_one(x: int) -> int:\\n    return x + 1\\n\\nrunnable = RunnableLambda(add_one)\\n\\nprint(runnable.get_input_jsonschema())\\n\\n\\n\\nAdded in version 0.3.0.\\n\\n\\n\\n\\nget_input_schema(\\n\\nconfig: RunnableConfig | None = None,\\n\\n) â†’ type[BaseModel][source]#\\nGet a pydantic model that can be used to validate input to the Runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic input schema that depends on which\\nconfiguration the Runnable is invoked with.\\nThis method allows to get an input schema for a specific configuration.\\n\\nParameters:\\nconfig (RunnableConfig | None) â€“ A config to use when generating the schema.\\n\\nReturns:\\nA pydantic model that can be used to validate input.\\n\\nReturn type:\\ntype[BaseModel]\\n\\n\\n\\n\\n\\nget_name(\\n\\nsuffix: str | None = None,\\n*,\\nname: str | None = None,\\n\\n) â†’ str[source]#\\nGet the name of the Runnable.\\n\\nParameters:\\n\\nsuffix (str | None)\\nname (str | None)\\n\\n\\nReturn type:\\nstr\\n\\n\\n\\n\\n\\nget_output_jsonschema(\\n\\nconfig: RunnableConfig | None = None,\\n\\n) â†’ dict[str, Any][source]#\\nGet a JSON schema that represents the output of the Runnable.\\n\\nParameters:\\nconfig (RunnableConfig | None) â€“ A config to use when generating the schema.\\n\\nReturns:\\nA JSON schema that represents the output of the Runnable.\\n\\nReturn type:\\ndict[str, Any]\\n\\n\\nExample\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef add_one(x: int) -> int:\\n    return x + 1\\n\\nrunnable = RunnableLambda(add_one)\\n\\nprint(runnable.get_output_jsonschema())\\n\\n\\n\\nAdded in version 0.3.0.\\n\\n\\n\\n\\nget_output_schema(\\n\\nconfig: RunnableConfig | None = None,\\n\\n) â†’ type[BaseModel][source]#\\nGet a pydantic model that can be used to validate output to the Runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic output schema that depends on which\\nconfiguration the Runnable is invoked with.\\nThis method allows to get an output schema for a specific configuration.\\n\\nParameters:\\nconfig (RunnableConfig | None) â€“ A config to use when generating the schema.\\n\\nReturns:\\nA pydantic model that can be used to validate output.\\n\\nReturn type:\\ntype[BaseModel]\\n\\n\\n\\n\\n\\nget_prompts(\\n\\nconfig: RunnableConfig | None = None,\\n\\n) â†’ list[BasePromptTemplate][source]#\\nReturn a list of prompts used by this Runnable.\\n\\nParameters:\\nconfig (Optional[RunnableConfig])\\n\\nReturn type:\\nlist[BasePromptTemplate]\\n\\n\\n\\n\\n\\nabstractmethod invoke(\\n\\ninput: Input,\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any,\\n\\n) â†’ Output[source]#\\nTransform a single input into an output.\\n\\nParameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like \\'tags\\', \\'metadata\\' for\\ntracing purposes, \\'max_concurrency\\' for controlling how much work to\\ndo in parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nkwargs (Any)\\n\\n\\nReturns:\\nThe output of the Runnable.\\n\\nReturn type:\\nOutput\\n\\n\\n\\n\\n\\nmap() â†’ Runnable[list[Input], list[Output]][source]#\\nReturn a new Runnable that maps a list of inputs to a list of outputs.\\nCalls invoke() with each input.\\n\\nReturns:\\nA new Runnable that maps a list of inputs to a list of outputs.\\n\\nReturn type:\\nRunnable[list[Input], list[Output]]\\n\\n\\nExample\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef _lambda(x: int) -> int:\\n    return x + 1\\n\\nrunnable = RunnableLambda(_lambda)\\nprint(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\\n\\n\\n\\n\\n\\npick(\\n\\nkeys: str | list[str],\\n\\n) â†’ RunnableSerializable[Any, Any][source]#\\nPick keys from the output dict of this Runnable.\\n\\nPick single key:import json\\n\\nfrom langchain_core.runnables import RunnableLambda, RunnableMap\\n\\nas_str = RunnableLambda(str)\\nas_json = RunnableLambda(json.loads)\\nchain = RunnableMap(str=as_str, json=as_json)\\n\\nchain.invoke(\"[1, 2, 3]\")\\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\\n\\njson_only_chain = chain.pick(\"json\")\\njson_only_chain.invoke(\"[1, 2, 3]\")\\n# -> [1, 2, 3]\\n\\n\\n\\nPick list of keys:from typing import Any\\n\\nimport json\\n\\nfrom langchain_core.runnables import RunnableLambda, RunnableMap\\n\\nas_str = RunnableLambda(str)\\nas_json = RunnableLambda(json.loads)\\ndef as_bytes(x: Any) -> bytes:\\n    return bytes(x, \"utf-8\")\\n\\nchain = RunnableMap(\\n    str=as_str,\\n    json=as_json,\\n    bytes=RunnableLambda(as_bytes)\\n)\\n\\nchain.invoke(\"[1, 2, 3]\")\\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\\n\\njson_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\\njson_and_bytes_chain.invoke(\"[1, 2, 3]\")\\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\\n\\n\\n\\n\\n\\nParameters:\\nkeys (str | list[str])\\n\\nReturn type:\\nRunnableSerializable[Any, Any]\\n\\n\\n\\n\\n\\npipe(\\n\\n*others: Runnable[Any, Other] | Callable[[Any], Other],\\nname: str | None = None,\\n\\n) â†’ RunnableSerializable[-Input, Other][source]#\\nCompose this Runnable with Runnable-like objects to make a RunnableSequence.\\nEquivalent to RunnableSequence(self, *others) or self | others[0] | â€¦\\nExample\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef add_one(x: int) -> int:\\n    return x + 1\\n\\ndef mul_two(x: int) -> int:\\n    return x * 2\\n\\nrunnable_1 = RunnableLambda(add_one)\\nrunnable_2 = RunnableLambda(mul_two)\\nsequence = runnable_1.pipe(runnable_2)\\n# Or equivalently:\\n# sequence = runnable_1 | runnable_2\\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\\nsequence.invoke(1)\\nawait sequence.ainvoke(1)\\n# -> 4\\n\\nsequence.batch([1, 2, 3])\\nawait sequence.abatch([1, 2, 3])\\n# -> [4, 6, 8]\\n\\n\\n\\nParameters:\\n\\nothers (Runnable[Any, Other] | Callable[[Any], Other])\\nname (str | None)\\n\\n\\nReturn type:\\nRunnableSerializable[-Input, ~Other]\\n\\n\\n\\n\\n\\nstream(\\n\\ninput: Input,\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any | None,\\n\\n) â†’ Iterator[Output][source]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]\\n\\n\\n\\n\\n\\ntransform(\\n\\ninput: Iterator[Input],\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any | None,\\n\\n) â†’ Iterator[Output][source]#\\nDefault implementation of transform, which buffers input and calls astream.\\nSubclasses should override this method if they can start producing output while\\ninput is still being generated.\\n\\nParameters:\\n\\ninput (Iterator[Input]) â€“ An iterator of inputs to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]\\n\\n\\n\\n\\n\\nwith_alisteners(\\n\\n*,\\non_start: AsyncListener | None = None,\\non_end: AsyncListener | None = None,\\non_error: AsyncListener | None = None,\\n\\n) â†’ Runnable[Input, Output][source]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[AsyncListener]) â€“ Called asynchronously before the Runnable starts running,\\nwith the Run object. Defaults to None.\\non_end (Optional[AsyncListener]) â€“ Called asynchronously after the Runnable finishes running,\\nwith the Run object. Defaults to None.\\non_error (Optional[AsyncListener]) â€“ Called asynchronously if the Runnable throws an error,\\nwith the Run object. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda, Runnable\\nfrom datetime import datetime, timezone\\nimport time\\nimport asyncio\\n\\ndef format_t(timestamp: float) -> str:\\n    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2025-03-01T07:05:22.875378+00:00\\non start callback starts at 2025-03-01T07:05:22.875495+00:00\\non start callback ends at 2025-03-01T07:05:25.878862+00:00\\non start callback ends at 2025-03-01T07:05:25.878947+00:00\\nRunnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\\nRunnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\\nRunnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\\non end callback starts at 2025-03-01T07:05:27.882360+00:00\\nRunnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\\non end callback starts at 2025-03-01T07:05:28.882428+00:00\\non end callback ends at 2025-03-01T07:05:29.883893+00:00\\non end callback ends at 2025-03-01T07:05:30.884831+00:00\\n\\n\\n\\n\\n\\nwith_config(\\n\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any,\\n\\n) â†’ Runnable[Input, Output][source]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) â€“ The config to bind to the Runnable.\\nkwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), exception_key: Optional[str] = None) â†’ RunnableWithFallbacksT[Input, Output][source]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\n\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.\\nexception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\n\\n\\n\\nwith_listeners(\\n\\n*,\\non_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\non_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\non_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\n\\n) â†’ Runnable[Input, Output][source]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called before the Runnable starts running, with the Run object.\\nDefaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called after the Runnable finishes running, with the Run object.\\nDefaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called if the Runnable throws an error, with the Run object.\\nDefaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\n\\n\\n\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, exponential_jitter_params: Optional[ExponentialJitterParams] = None, stop_after_attempt: int = 3) â†’ Runnable[Input, Output][source]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) â€“ Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) â€“ The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\nexponential_jitter_params (Optional[ExponentialJitterParams]) â€“ Parameters for\\ntenacity.wait_exponential_jitter. Namely: initial, max,\\nexp_base, and jitter (all float values).\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\n\\n\\n\\n\\nwith_types(\\n\\n*,\\ninput_type: type[Input] | None = None,\\noutput_type: type[Output] | None = None,\\n\\n) â†’ Runnable[Input, Output][source]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) â€“ The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) â€“ The output type to bind to the Runnable. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nExamples using Runnable\\n\\nHow to add a human-in-the-loop for tools\\nHow to create a dynamic (self-constructing) chain\\nHow to handle tool errors\\n\\n\\n\\n\\n\\n\\n\\n\\n On this page\\n  \\n\\n\\nRunnable\\nabatch()\\nabatch_as_completed()\\nainvoke()\\nas_tool()\\nassign()\\nastream()\\nastream_events()\\nastream_log()\\natransform()\\nbatch()\\nbatch_as_completed()\\nbind()\\nconfig_schema()\\nget_config_jsonschema()\\nget_graph()\\nget_input_jsonschema()\\nget_input_schema()\\nget_name()\\nget_output_jsonschema()\\nget_output_schema()\\nget_prompts()\\ninvoke()\\nmap()\\npick()\\npipe()\\nstream()\\ntransform()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_types()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      Â© Copyright 2025, LangChain Inc.\\n      \\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/api_reference/core/agents.html', 'title': 'agents â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\nagents â€” ðŸ¦œðŸ”— LangChain  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\nBack to top\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages\\n\\nCore\\nagents\\nAgentAction\\nAgentActionMessageLog\\nAgentFinish\\nAgentStep\\n\\n\\nbeta\\ncaches\\ncallbacks\\nchat_history\\nchat_loaders\\nchat_sessions\\ndocument_loaders\\ndocuments\\nembeddings\\nexample_selectors\\nexceptions\\nglobals\\nindexing\\nlanguage_models\\nload\\nmessages\\noutput_parsers\\noutputs\\nprompt_values\\nprompts\\nrate_limiters\\nretrievers\\nrunnables\\nstores\\nstructured_query\\nsys_info\\ntools\\ntracers\\nutils\\nvectorstores\\n\\n\\nLangchain\\nText Splitters\\nCommunity\\nExperimental\\n\\nIntegrations\\n\\nAnthropic\\nAstraDB\\nAWS\\nAzure Ai\\nAzure Dynamic Sessions\\nCerebras\\nChroma\\nCli\\nCohere\\nDb2\\nDeepseek\\nElasticsearch\\nExa\\nFireworks\\nGoogle Community\\nGoogle GenAI\\nGoogle VertexAI\\nGroq\\nHuggingface\\nIBM\\nLangchain_V1\\nMilvus\\nMistralAI\\nMongoDB\\nNeo4J\\nNomic\\nNvidia Ai Endpoints\\nOllama\\nOpenAI\\nPerplexity\\nPinecone\\nPostgres\\nPrompty\\nQdrant\\nRedis\\nSema4\\nSnowflake\\nSqlserver\\nStandard Tests\\nTavily\\nTogether\\nUnstructured\\nUpstage\\nWeaviate\\nXAI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Python API Reference\\nlangchain-core: 0.3.72\\nagents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nagents#\\nSchema definitions for representing agent actions, observations, and return values.\\nATTENTION The schema definitions are provided for backwards compatibility.\\n\\nNew agents should be built using the langgraph library\\n(langchain-ai/langgraph)), which provides a simpler\\nand more flexible way to define agents.\\nPlease see the migration guide for information on how to migrate existing\\nagents to modern langgraph agents:\\nhttps://python.langchain.com/docs/how_to/migrate_agent/\\n\\nAgents use language models to choose a sequence of actions to take.\\nA basic agent works in the following manner:\\n\\nGiven a prompt an agent uses an LLM to request an action to take (e.g., a tool to run).\\nThe agent executes the action (e.g., runs the tool), and receives an observation.\\nThe agent returns the observation to the LLM, which can then be used to generate the next action.\\nWhen the agent reaches a stopping condition, it returns a final return value.\\n\\nThe schemas for the agents themselves are defined in langchain.agents.agent.\\nClasses\\n\\n\\nagents.AgentAction\\nRepresents a request to execute an action by an agent.\\n\\nagents.AgentActionMessageLog\\nRepresentation of an action to be executed by an agent.\\n\\nagents.AgentFinish\\nFinal return value of an ActionAgent.\\n\\nagents.AgentStep\\nResult of running an AgentAction.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      Â© Copyright 2025, LangChain Inc.\\n      \\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c659853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b3d81b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\AI_AGENTS\\Python_N_Env\\LangGraph\\graphenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000 , chunk_overlap = 100)\n",
    "docs_splits = text_splitter.split_documents(docs_list)\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=docs_splits,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8bec48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_langchain= vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a49f6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool_langchain= create_retriever_tool(\n",
    "    retriever_langchain,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b1ba8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_langgraph = [\n",
    "    'https://www.langchain.com/langgraph',\n",
    "    'https://python.langchain.com/docs/tutorials/agents/',\n",
    "    'https://python.langchain.com/docs/versions/migrating_memory/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24f92a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://www.langchain.com/langgraph', 'title': 'LangGraph', 'description': 'Build controllable agents with LangGraph, our low-level agent orchestration framework. Deploy and scale with LangGraph Platform, with APIs for state management, a visual studio for debugging, and multiple deployment options.', 'language': 'en'}, page_content='LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nResources HubBlogCustomer StoriesLangChain AcademyCommunityExpertsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upBalance agent control with agencyGain control with LangGraph to design agents that reliably handle complex tasks.Start building\\n\\nIntroduction to LangGraphLearn the basics of LangGraph in this LangChain Academy Course. You\\'ll learn how to build agents that automate real-world tasks with LangGraph orchestration.Enroll for freeBook enterprise trainingNominate your team for a hands-on training and learn how to build reliable agents with LangGraphLearn More\\n\\n\\nTrusted by companies shaping the future of agentsSee LangGraph use cases in production\\n\\n\\nControllable cognitive architecture for any taskLangGraph\\'s flexible framework supports diverse control flows â€“ single agent, multi-agent, hierarchical, sequential â€“ and robustly handles realistic, complex scenarios. Ensure reliability with easy-to-add moderation and quality loops that prevent agents from veering off course.Use LangGraph Platform to templatize your cognitive architecture so that tools, prompts, and models are easily configurable with LangGraph Platform Assistants.See the docs\\n\\n\\nThousands of companies build AI apps better with LangChain products.Read our select customer stories.Designed for human-agent collaborationWith built-in statefulness, LangGraph agents seamlessly collaborate with humans by writing drafts for review and awaiting approval before acting. Easily inspect the agentâ€™s actions and \"time-travel\" to roll back and take a different action to correct course.Read a conceptual guide\\n\\n\\nHow does LangGraph help?Guide, moderate, and control your agent with human-in-the-loop.Prevent agents from veering off course with easy-to-add moderation and quality controls. Add human-in-the-loop checks to steer and approve agent actions.Learn how to add human-in-the-loop\\n\\n\\nBuild expressive, customizable agent workflows.LangGraphâ€™s low-level primitives provide the flexibility needed to create fully customizable agents. Design diverse control flows â€” single, multi-agent, hierarchical â€” all using one framework.See different agent architectures\\n\\n\\nPersist context for long-term interactions.LangGraphâ€™s built-in memory stores conversation histories and maintains context over time, enabling rich, personalized interactions across sessions.Learn about agent memory\\n\\n\\nFirst-class streaming for better UX design.Bridge user expectations and agent capabilities with native token-by-token streaming, showing agent reasoning and actions in real time.See how to use streaming\\n\\n\\nFirst class streaming support for better UX designBridge user expectations and agent capabilities with native token-by-token streaming and streaming of intermediate steps, helpful for showing agent reasoning and actions back to the user as they happen. Use LangGraph Platform\\'s API to deliver dynamic and interactive user experiences.Learn more\\n\\n\\nIntroduction to LangGraphLearn the basics of LangGraph in this LangChain Academy Course. You\\'ll learn how to build agents that automate real-world tasks with LangGraph orchestration.Enroll for freeBook a trainingDeploy agents at scale, monitor carefully, iterate boldlyDesign agent-driven user experiences with LangGraph Platform\\'s APIs. Quickly deploy and scale your application with infrastructure built for agents. Choose from multiple deployment options. \\n\\n\\n\\nFault-tolerant scalabilityHandle large workloads gracefully with horizontally-scaling servers, task queues, and built-in persistence. Enhance resilience with intelligent caching and automated retries.\\n\\n\\n\\nDynamic APIs for designing agent experienceCraft personalized user experiences with APIs featuring long-term memory to recall information across conversation sessions. Track, update, and rewind your app\\'s state for easy human steering and interaction. Kick off long-running background jobs for research-style or multi-step work.\\n\\n\\n\\nIntegrated developer experienceSimplify prototyping, debugging, and sharing of agents in our visual LangGraph Studio. Deploy your application with 1-click deploy with our SaaS offering or within your own VPC. Then, monitor app performance with LangSmith.Without LangGraph PlatformWrite your own API endpoints for human-in-the-loop, background jobs, and more. Manage state and checkpointing. \\u2028Handle horizontal scaling and engineer fault tolerance. Continual maintenance and on-call.With LangGraph PlatformFocus on the app logic, not the infrastructure. Full batteries included â€” APIs, scalability, streaming, built in.Developers trust LangGraph to build reliable agents.LangGraph helps teams of all sizes, across all industries, build reliable agents ready for production.Hear how industry leaders use LangGraph\\n\\n\\nâ€œLangChain is streets ahead with what they\\'ve put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€Garrett SpongPrincipal SWE â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€Andres TorresSr. Solutions Architectâ€œIt\\'s easy to build the prototype of a coding agent, but deceptively hard to improve its reliability. Replit wants to give a coding agent to millions of users â€” reliability is our top priority, and will remain so for a long time. LangGraph is giving us the control and ergonomics we need to build and ship powerful coding agents.â€â€œAs Ally advances its exploration of Generative AI, Michele CatastaPresidentâ€œAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.â€â€œAs Ally advances its exploration of Generative AI, Sathish MuthukrishnanChief Information, Data and Digital Officerâ€œLangChain is streets ahead with what they\\'ve put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€Garrett SpongPrincipal SWE â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€Andres TorresSr. Solutions Architectâ€œIt\\'s easy to build the prototype of a coding agent, but deceptively hard to improve its reliability. Replit wants to give a coding agent to millions of users â€” reliability is our top priority, and will remain so for a long time. LangGraph is giving us the control and ergonomics we need to build and ship powerful coding agents.â€â€œAs Ally advances its exploration of Generative AI, Michele CatastaPresidentâ€œAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.â€â€œAs Ally advances its exploration of Generative AI, Sathish MuthukrishnanChief Information, Data and Digital Officer\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph FAQsHow is LangGraph different from other agent frameworks?\\n\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a companyâ€™s needs. LangGraph provides a more expressive framework to handle companiesâ€™ unique tasks without restricting users to a single black-box cognitive architecture.Does LangGraph impact the performance of my app?\\n\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.Is LangGraph open source? Is it free?\\n\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.How are LangGraph and LangGraph Platform different?\\n\\nLangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.LangGraph (open source)LangGraph PlatformFeaturesDescriptionStateful orchestration framework for agentic applicationsScalable infrastructure for deploying LangGraph applicationsSDKsPython and JavaScriptPython and JavaScriptHTTP\\xa0APIsNoneYes - useful for retrieving & updating state or long-term memory, or creating a configurable assistantStreamingBasicDedicated mode for token-by-token messagesCheckpointerCommunity contributedSupported out-of-the-boxPersistence LayerSelf-managedManaged Postgres with efficient storageDeploymentSelf-managed- Cloud- Hybrid- Full self-hostedScalabilitySelf-managedAuto-scaling of task queues and serversFault-toleranceSelf-managedAutomated retriesConcurrency ControlSimple threadingSupports double-textingSchedulingNoneCron schedulingMonitoringOpt-in LangSmith integration for observabilityIntegrated with LangSmith for observabilityIDE integrationLangGraph Studio for DesktopLangGraph Studio for Desktop & CloudWhat are my deployment options for LangGraph Platform?\\n\\nWe currently have the following deployment options for LangGraph applications:\\u200dCloud SaaS:\\xa0Fully managed and hosted as part of LangSmith (our unified observability\\xa0& evals platform).\\xa0Deploy quickly, with automatic updates and zero maintenance. \\u200dHybrid (SaaS control plane, self-hosted data plane). No data leaves your VPC. Provisioning and scaling is managed as a service.\\u200dFully Self-Hosted: Deploy LangGraph entirely on your own infrastructure.\\u200dIf you want to try out a basic version of our LangGraph server in your environment, you can also self-host on our Developer plan and get up to 100k nodes executed per month for free.\\xa0Great for running hobbyist projects, with fewer features are available than in paid plans.\\u200dIs LangGraph Platform open source?\\n\\nNo. LangGraph Platform is proprietary software.\\u200dThere is a free, self-hosted version of LangGraph Platform with access to basic features. The Cloud SaaS deployment option is free while in beta, but will eventually be a paid service. We will always give ample notice before charging for a service and reward our early adopters with preferential pricing. The Bring Your Own Cloud (BYOC) and Self-Hosted Enterprise options are also paid services. Contact our sales team to learn more.\\u200dFor more information, see our LangGraph Platform pricing page.Ready to start shipping \\u2028reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Contact UsSign UpProductsLangChainLangSmithLangGraphAgentsEvaluationRetrievalResourcesPython DocsJS/TS DocsGitHubIntegrationsChangelogCommunityLangSmith Trust PortalCompanyAboutCareersBlogTwitterLinkedInYouTubeMarketing AssetsSign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/agents/', 'title': 'Build an Agent | ðŸ¦œï¸ðŸ”— LangChain', 'description': 'LangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild an Agent | ðŸ¦œï¸ðŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ðŸ’¬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?EcosystemðŸ¦œðŸ› ï¸ LangSmithðŸ¦œðŸ•¸ï¸ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild an AgentOn this pageBuild an Agent\\nLangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action.\\nAfter executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via tool-calling.\\nIn this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it.\\nEnd-to-end agent\\u200b\\nThe code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot.\\nIn the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this!\\n# Import relevant functionalityfrom langchain.chat_models import init_chat_modelfrom langchain_tavily import TavilySearchfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.prebuilt import create_react_agent# Create the agentmemory = MemorySaver()model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")search = TavilySearch(max_results=2)tools = [search]agent_executor = create_react_agent(model, tools, checkpointer=memory)API Reference:MemorySaver | create_react_agent\\n# Use the agentconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_message = {    \"role\": \"user\",    \"content\": \"Hi, I\\'m Bob and I live in SF.\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob and I live in SF.==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I notice you\\'ve introduced yourself and mentioned you live in SF (San Francisco), but you haven\\'t asked a specific question or made a request that requires the use of any tools. Is there something specific you\\'d like to know about San Francisco or any other topic? I\\'d be happy to help you find information using the available search tools.\\ninput_message = {    \"role\": \"user\",    \"content\": \"What\\'s the weather where I live?\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s the weather where I live?==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \\'Let me search for current weather information in San Francisco.\\', \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_011kSdheoJp8THURoLmeLtZo\\', \\'input\\': {\\'query\\': \\'current weather San Francisco CA\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_011kSdheoJp8THURoLmeLtZo) Call ID: toolu_011kSdheoJp8THURoLmeLtZo  Args:    query: current weather San Francisco CA=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco CA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.944705, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ðŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.86441374, \"raw_content\": null}], \"response_time\": 2.34}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1Â°F (11.7Â°C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9 milesThis is quite typical weather for San Francisco, with the characteristic fog that the city is known for. Would you like to know anything else about the weather or San Francisco in general?\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n%pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nTavily\\u200b\\nWe will be using Tavily (a search engine) as a tool.\\nIn order to use it, you will need to get and set an API key:\\nexport TAVILY_API_KEY=\"...\"\\nOr, if in a notebook, you can set it with:\\nimport getpassimport osos.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\\nDefine tools\\u200b\\nWe first need to create the tools we want to use. Our main tool of choice will be Tavily - a search engine. We can use the dedicated langchain-tavily integration package to easily use Tavily search engine as tool with LangChain.\\nfrom langchain_tavily import TavilySearchsearch = TavilySearch(max_results=2)search_results = search.invoke(\"What is the weather in SF\")print(search_results)# If we want, we can create other tools.# Once we have all the tools we want, we can put them in a list that we will reference later.tools = [search]\\n{\\'query\\': \\'What is the weather in SF\\', \\'follow_up_questions\\': None, \\'answer\\': None, \\'images\\': [], \\'results\\': [{\\'title\\': \\'Weather in San Francisco, CA\\', \\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \\'score\\': 0.9185379, \\'raw_content\\': None}, {\\'title\\': \\'Weather in San Francisco in June 2025\\', \\'url\\': \\'https://world-weather.info/forecast/usa/san_francisco/june-2025/\\', \\'content\\': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month *   Weather in San Francisco Weather in San Francisco in June 2025 *   1 +63Â° +55Â° *   2 +66Â° +54Â° *   3 +66Â° +55Â° *   4 +66Â° +54Â° *   5 +66Â° +55Â° *   6 +66Â° +57Â° *   7 +64Â° +55Â° *   8 +63Â° +55Â° *   9 +63Â° +54Â° *   10 +59Â° +54Â° *   11 +59Â° +54Â° *   12 +61Â° +54Â° Weather in Washington, D.C.**+68Â°** Sacramento**+81Â°** Pleasanton**+72Â°** Redwood City**+68Â°** San Leandro**+61Â°** San Mateo**+64Â°** San Rafael**+70Â°** San Ramon**+64Â°** South San Francisco**+61Â°** Daly City**+59Â°** Wilder**+66Â°** Woodacre**+70Â°** world\\'s temperature today Colchani day+50Â°F night+16Â°F Az Zubayr day+124Â°F night+93Â°F Weather forecast on your site Install _San Francisco_ +61Â° Temperature units\", \\'score\\': 0.7978881, \\'raw_content\\': None}], \\'response_time\\': 2.62}\\ntipIn many applications, you may want to define custom tools. LangChain supports custom\\ntool creation via Python functions and other means. Refer to the\\nHow to create tools guide for details.\\nUsing Language Models\\u200b\\nNext, let\\'s learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\\n\\nSelect chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nYou can call the language model by passing in a list of messages. By default, the response is a content string.\\nquery = \"Hi!\"response = model.invoke([{\"role\": \"user\", \"content\": query}])response.text()\\n\\'Hello! How can I help you today?\\'\\nWe can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools\\nmodel_with_tools = model.bind_tools(tools)\\nWe can now call the model. Let\\'s first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field.\\nquery = \"Hi!\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\\\n\")print(f\"Tool calls: {response.tool_calls}\")\\nMessage content: Hello! I\\'m here to help you. I have access to a powerful search tool that can help answer questions and find information about various topics. What would you like to know about?Feel free to ask any question or request information, and I\\'ll do my best to assist you using the available tools.Tool calls: []\\nNow, let\\'s try calling it with some input that would expect a tool to be called.\\nquery = \"Search for the weather in SF\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\\\n\")print(f\"Tool calls: {response.tool_calls}\")\\nMessage content: I\\'ll help you search for information about the weather in San Francisco.Tool calls: [{\\'name\\': \\'tavily_search\\', \\'args\\': {\\'query\\': \\'current weather San Francisco\\'}, \\'id\\': \\'toolu_015gdPn1jbB2Z21DmN2RAnti\\', \\'type\\': \\'tool_call\\'}]\\nWe can see that there\\'s now no text content, but there is a tool call! It wants us to call the Tavily Search tool.\\nThis isn\\'t calling that tool yet - it\\'s just telling us to. In order to actually call it, we\\'ll want to create our agent.\\nCreate the agent\\u200b\\nNow that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent.\\nCurrently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\\nNow, we can initialize the agent with the LLM and the tools.\\nNote that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood.\\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(model, tools)API Reference:create_react_agent\\nRun the agent\\u200b\\nWe can now run the agent with a few queries! Note that for now, these are all stateless queries (it won\\'t remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).\\nFirst up, let\\'s see how it responds when there\\'s no need to call a tool:\\ninput_message = {\"role\": \"user\", \"content\": \"Hi!\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello! I\\'m here to help you with your questions using the available search tools. Please feel free to ask any question, and I\\'ll do my best to find relevant and accurate information for you.\\nIn order to see exactly what is happening under the hood (and to make sure it\\'s not calling a tool) we can take a look at the LangSmith trace\\nLet\\'s now try it out on an example where it should be invoking the tool\\ninput_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Search for the weather in SF==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \"I\\'ll help you search for weather information in San Francisco. Let me use the search engine to find current weather conditions.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01WWcXGnArosybujpKzdmARZ\\', \\'input\\': {\\'query\\': \\'current weather San Francisco SF\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_01WWcXGnArosybujpKzdmARZ) Call ID: toolu_01WWcXGnArosybujpKzdmARZ  Args:    query: current weather San Francisco SF=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco SF\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.885373, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ðŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.8830044, \"raw_content\": null}], \"response_time\": 2.6}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1Â°F (11.7Â°C)- Conditions: Foggy- Wind: 4.0 mph from the SW- Humidity: 86%- Visibility: 9.0 milesThe weather appears to be typical for San Francisco, with morning fog and mild temperatures. The \"feels like\" temperature is 52.4Â°F (11.3Â°C).\\nWe can check out the LangSmith trace to make sure it\\'s calling the search tool effectively.\\nStreaming Messages\\u200b\\nWe\\'ve seen how the agent can be called with .invoke to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nfor step in agent_executor.stream({\"messages\": [input_message]}, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Search for the weather in SF==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \"I\\'ll help you search for information about the weather in San Francisco.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01DCPnJES53Fcr7YWnZ47kDG\\', \\'input\\': {\\'query\\': \\'current weather San Francisco\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_01DCPnJES53Fcr7YWnZ47kDG) Call ID: toolu_01DCPnJES53Fcr7YWnZ47kDG  Args:    query: current weather San Francisco=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168506, \\'localtime\\': \\'2025-06-17 06:55\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.9542825, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ðŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.8638634, \"raw_content\": null}], \"response_time\": 2.57}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1Â°F (11.7Â°C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9.0 miles- Feels like: 52.4Â°F (11.3Â°C)This is quite typical weather for San Francisco, which is known for its fog, especially during the morning hours. The city\\'s proximity to the ocean and unique geographical features often result in mild temperatures and foggy conditions.\\nStreaming tokens\\u200b\\nIn addition to streaming back messages, it is also useful to stream back tokens.\\nWe can do this by specifying stream_mode=\"messages\".\\n::: note\\nBelow we use message.text(), which requires langchain-core>=0.3.37.\\n:::\\nfor step, metadata in agent_executor.stream(    {\"messages\": [input_message]}, stream_mode=\"messages\"):    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):        print(text, end=\"|\")\\nI|\\'ll help you search for information| about the weather in San Francisco.|Base|d on the search results, here|\\'s the current weather in| San Francisco:-| Temperature: 53.1Â°F (|11.7Â°C)-| Condition: Foggy- Wind:| 4.0 mph from| the Southwest- Humidity|: 86%|- Visibility: 9|.0 miles- Pressure: |30.02 in|HgThe weather| is characteristic of San Francisco, with| foggy conditions and mild temperatures|. The \"feels like\" temperature is slightly| lower at 52.4|Â°F (11.|3Â°C)| due to the wind chill effect|.|\\nAdding in memory\\u200b\\nAs mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from).\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()API Reference:MemorySaver\\nagent_executor = create_react_agent(model, tools, checkpointer=memory)config = {\"configurable\": {\"thread_id\": \"abc123\"}}\\ninput_message = {\"role\": \"user\", \"content\": \"Hi, I\\'m Bob!\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I\\'m an AI assistant who can help you search for information using specialized search tools. Is there anything specific you\\'d like to know about or search for? I\\'m happy to help you find accurate and up-to-date information on various topics.\\ninput_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it.\\nExample LangSmith trace\\nIf you want to start a new conversation, all you have to do is change the thread_id used\\nconfig = {\"configurable\": {\"thread_id\": \"xyz123\"}}input_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================I apologize, but I don\\'t have access to any tools that would tell me your name. I can only assist you with searching for publicly available information using the tavily_search function. I don\\'t have access to personal information about users. If you\\'d like to tell me your name, I\\'ll be happy to address you by it.\\nConclusion\\u200b\\nThat\\'s a wrap! In this quick start we covered how to create a simple agent.\\nWe\\'ve then shown how to stream back a response - not only with the intermediate steps, but also tokens!\\nWe\\'ve also added in memory so you can have a conversation with them.\\nAgents are a complex topic with lots to learn!\\nFor more information on Agents, please check out the LangGraph documentation. This has it\\'s own set of concepts, tutorials, and how-to guides.Edit this pagePreviousBuild an Extraction ChainNextTaggingEnd-to-end agentSetupJupyter NotebookInstallationLangSmithTavilyDefine toolsUsing Language ModelsCreate the agentRun the agentStreaming MessagesStreaming tokensAdding in memoryConclusionCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/versions/migrating_memory/', 'title': 'How to migrate to LangGraph memory | ðŸ¦œï¸ðŸ”— LangChain', 'description': 'As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into their LangChain application.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nHow to migrate to LangGraph memory | ðŸ¦œï¸ðŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ðŸ’¬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?EcosystemðŸ¦œðŸ› ï¸ LangSmithðŸ¦œðŸ•¸ï¸ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyVersionsUpgrading to LangGraph memoryOn this pageHow to migrate to LangGraph memory\\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into their LangChain application.\\n\\nUsers that rely on RunnableWithMessageHistory or BaseChatMessageHistory do not need to make any changes, but are encouraged to consider using LangGraph for more complex use cases.\\nUsers that rely on deprecated memory abstractions from LangChain 0.0.x should follow this guide to upgrade to the new LangGraph persistence feature in LangChain 0.3.x.\\n\\nWhy use LangGraph for memory?\\u200b\\nThe main advantages of persistence in LangGraph are:\\n\\nBuilt-in support for multiple users and conversations, which is a typical requirement for real-world conversational AI applications.\\nAbility to save and resume complex conversations at any point. This helps with:\\n\\nError recovery\\nAllowing human intervention in AI workflows\\nExploring different conversation paths (\"time travel\")\\n\\n\\nFull compatibility with both traditional language models and modern chat models. Early memory implementations in LangChain weren\\'t designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state.\\nHighly customizable, allowing you to fully control how memory works and use different storage backends.\\n\\nEvolution of memory in LangChain\\u200b\\nThe concept of memory has evolved significantly in LangChain since its initial release.\\nLangChain 0.0.x memory\\u200b\\nBroadly speaking, LangChain 0.0.x memory was used to handle three main use cases:\\nUse CaseExampleManaging conversation historyKeep only the last n turns of the conversation between the user and the AI.Extraction of structured informationExtract structured information from the conversation history, such as a list of facts learned about the user.Composite memory implementationsCombine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation.\\nWhile the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.\\nMost of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.\\nRunnableWithMessageHistory and BaseChatMessageHistory\\u200b\\nnotePlease see How to use BaseChatMessageHistory with LangGraph, if you would like to use BaseChatMessageHistory (with or without RunnableWithMessageHistory) in LangGraph.\\nAs of LangChain v0.1, we started recommending that users rely primarily on BaseChatMessageHistory. BaseChatMessageHistory serves\\nas a simple persistence for storing and retrieving messages in a conversation.\\nAt that time, the only option for orchestrating LangChain chains was via LCEL. To incorporate memory with LCEL, users had to use the RunnableWithMessageHistory interface. While sufficient for basic chat applications, many users found the API unintuitive and challenging to use.\\nAs of LangChain v0.3, we recommend that new code takes advantage of LangGraph for both orchestration and persistence:\\n\\nOrchestration: In LangGraph, users define graphs that specify the flow of the application. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nPersistence: Users can rely on LangGraph\\'s persistence to store and retrieve data. LangGraph persistence is extremely flexible and can support a much wider range of use cases than the RunnableWithMessageHistory interface.\\n\\nimportantIf you have been using RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating either functionality in the near future. This functionality is sufficient for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.\\nMigrations\\u200b\\nPrerequisitesThese guides assume some familiarity with the following concepts:\\nLangGraph\\nv0.0.x Memory\\nHow to add persistence (\"memory\") to your graph\\n\\n1. Managing conversation history\\u200b\\nThe goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.\\nOften this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.\\nMemory classes that fall into this category include:\\nMemory TypeHow to MigrateDescriptionConversationBufferMemoryLink to Migration GuideA basic memory implementation that simply stores the conversation history.ConversationStringBufferMemoryLink to Migration GuideA special case of ConversationBufferMemory designed for LLMs and no longer relevant.ConversationBufferWindowMemoryLink to Migration GuideKeeps the last n turns of the conversation. Drops the oldest turn when the buffer is full.ConversationTokenBufferMemoryLink to Migration GuideKeeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.ConversationSummaryMemoryLink to Migration GuideContinually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.ConversationSummaryBufferMemoryLink to Migration GuideProvides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.VectorStoreRetrieverMemorySee related long-term memory agent tutorialStores the conversation history in a vector store and retrieves the most relevant parts of past conversation based on the input.\\n2. Extraction of structured information from the conversation history\\u200b\\nPlease see long-term memory agent tutorial implements an agent that can extract structured information from the conversation history.\\nMemory classes that fall into this category include:\\nMemory TypeDescriptionBaseEntityStoreAn abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as a dictionary of key-value pairs.ConversationEntityMemoryCombines the ability to summarize the conversation while extracting structured information from the conversation history.\\nAnd specific backend implementations of abstractions:\\nMemory TypeDescriptionInMemoryEntityStoreAn implementation of BaseEntityStore that stores the information in the literal computer memory (RAM).RedisEntityStoreA specific implementation of BaseEntityStore that uses Redis as the backend.SQLiteEntityStoreA specific implementation of BaseEntityStore that uses SQLite as the backend.UpstashRedisEntityStoreA specific implementation of BaseEntityStore that uses Upstash as the backend.\\nThese abstractions have received limited development since their initial release. This is because they generally require significant customization for a specific application to be effective, making\\nthem less widely used than the conversation history management abstractions.\\nFor this reason, there are no migration guides for these abstractions. If you\\'re struggling to migrate an application\\nthat relies on these abstractions, please:\\n\\nPlease review this Long-term memory agent tutorial which should provide a good starting point for how to extract structured information from the conversation history.\\nIf you\\'re still struggling, please open an issue on the LangChain GitHub repository, explain your use case, and we\\'ll try to provide more guidance on how to migrate these abstractions.\\n\\nThe general strategy for extracting structured information from the conversation history is to use a chat model with tool calling capabilities to extract structured information from the conversation history.\\nThe extracted information can then be saved into an appropriate data structure (e.g., a dictionary), and information from it can be retrieved and added into the prompt as needed.\\n3. Implementations that provide composite logic on top of one or more memory implementations\\u200b\\nMemory classes that fall into this category include:\\nMemory TypeDescriptionCombinedMemoryThis abstraction accepted a list of BaseMemory and fetched relevant memory information from each of them based on the input.SimpleMemoryUsed to add read-only hard-coded context. Users can simply write this information into the prompt.ReadOnlySharedMemoryProvided a read-only view of an existing BaseMemory implementation.\\nThese implementations did not seem to be used widely or provide significant value. Users should be able\\nto re-implement these without too much difficulty in custom code.\\nRelated Resources\\u200b\\nExplore persistence with LangGraph:\\n\\nLangGraph quickstart tutorial\\nHow to add persistence (\"memory\") to your graph\\nHow to manage conversation history\\nHow to add summary of the conversation history\\n\\nAdd persistence with simple LCEL (favor langgraph for more complex use cases):\\n\\nHow to add message history\\n\\nWorking with message history:\\n\\nHow to trim messages\\nHow to filter messages\\nHow to merge message runs\\nEdit this pagePreviousMigrating from StuffDocumentsChainNextHow to migrate to LangGraph memoryWhy use LangGraph for memory?Evolution of memory in LangChainLangChain 0.0.x memoryRunnableWithMessageHistory and BaseChatMessageHistoryMigrations1. Managing conversation history2. Extraction of structured information from the conversation history3. Implementations that provide composite logic on top of one or more memory implementationsRelated ResourcesCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2025 LangChain, Inc.\\n\\n')]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls_langgraph]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d9c6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstorelanggraph=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "\n",
    "retriever_langGraph=vectorstorelanggraph.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20ef0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool_langgraph=create_retriever_tool(\n",
    "    retriever_langGraph,\n",
    "    \"retriever_vector_langGraph_blog\",\n",
    "    \"Search and run information about LangGraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47b413eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool_langgraph,retriever_tool_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fca2d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f051760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence , Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage , HumanMessage\n",
    "\n",
    "# from langgraph.graph.message import add_messag\n",
    "# es\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel , Field\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4890846f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello. How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 36, 'total_tokens': 46, 'completion_time': 0.018348265, 'prompt_time': 0.001731399, 'queue_time': 0.008412098, 'total_time': 0.020079664}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8164ab98-99e8-431d-8177-e6024391aa23-0', usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "model.invoke('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ad8f9",
   "metadata": {},
   "source": [
    "# **`Nodes Definition`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb50654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate the response based on the current state.Given question it will\n",
    "    decide whether to do a tool call or just end the response\n",
    "    Args:\n",
    "        state (messages) : Current state\n",
    "    Returns:\n",
    "        dict : updated state with agent response appended to the messages    \n",
    "    \"\"\"\n",
    "    print(\"__________________ CALL AGENT ___________________\")\n",
    "    model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "    llm_with_tool = model.bind_tools(tools)\n",
    "    msg = state['messages']\n",
    "    response = llm_with_tool.invoke(msg)\n",
    "    return {\"messages\" : [response]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ee5539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "def gradeDocument(state) -> Literal['generate' , 'rewrite']:\n",
    "    \"\"\"\n",
    "    Determine whether the retrieved document is relevant to the question of user or not\n",
    "    Args: \n",
    "        state (messages) : current state\n",
    "    Returns : \n",
    "        str : A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    class Grade(BaseModel):\n",
    "        \"\"\" Binary score for the Relevance of the document\"\"\"\n",
    "        binaryscore : str = Field(description=\"Relevance score 'Yes' or 'No'\")\n",
    "\n",
    "\n",
    "    model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "    model_str_par = model.with_structured_output(Grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    msg = state['messages']\n",
    "    question = msg[0].content\n",
    "    last_msg = msg[-1].content\n",
    "    chain = prompt | model_str_par\n",
    "    bin_res = chain.invoke({\"context\" : last_msg , \"question\" : question})\n",
    "    score = bin_res.binaryscore\n",
    "    if (score == \"Yes\" or score == 'yes'):\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "    elif (score =='No' or score == 'no'):\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "037d1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generaet Answer\n",
    "    Args:\n",
    "        state (messages) : The current state\n",
    "    Returns :\n",
    "        dict : updated message\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d289840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce the relevant document from the tool\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Get the original question from the user's message\n",
    "    question = messages[0].content\n",
    "\n",
    "    # Now create the prompt using the question\n",
    "    msg = [HumanMessage(content=f\"\"\"\n",
    "        Look at the input and try to reason about the underlying meaning and semantics.\n",
    "\n",
    "        Here is the Initial Question:\n",
    "        -------\n",
    "        {question}\n",
    "        -------\n",
    "\n",
    "        Formulate the Improved Question.\n",
    "    \"\"\")]\n",
    "\n",
    "    model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    return {\"messages\": [response]}  # or: {\"messages\": messages + [response]} if you want to preserve history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b2a5eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node('agent' , agent)\n",
    "builder.add_node('retrieve' , ToolNode(tools))\n",
    "builder.add_node('generate' , generate)\n",
    "builder.add_node('rewrite' , rewrite)\n",
    "\n",
    "builder.add_edge(START , 'agent')\n",
    "builder.add_conditional_edges('agent' , tools_condition,\n",
    "                              {\n",
    "                                  'tools' : 'retrieve', ## if there is need about tool call then it will do it o/w just LLM give the answer and that's it\n",
    "                                  END : END\n",
    "                              })\n",
    "\n",
    "builder.add_conditional_edges('retrieve' , gradeDocument)\n",
    "builder.add_edge('generate' , END)\n",
    "builder.add_edge('rewrite' , \"agent\")\n",
    "graph = builder.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fcc22a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________ CALL AGENT ___________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\AI_AGENTS\\Python_N_Env\\LangGraph\\graphenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "---TRANSFORM QUERY---\n",
      "__________________ CALL AGENT ___________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\AI_AGENTS\\Python_N_Env\\LangGraph\\graphenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "---TRANSFORM QUERY---\n",
      "__________________ CALL AGENT ___________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\AI_AGENTS\\Python_N_Env\\LangGraph\\graphenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "---TRANSFORM QUERY---\n",
      "__________________ CALL AGENT ___________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Computer Vision?', additional_kwargs={}, response_metadata={}, id='a68c16c9-6b07-4680-8076-88b53167bfb3'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '8e1keryx9', 'function': {'arguments': '{\"query\":\"What is Computer Vision?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 312, 'total_tokens': 334, 'completion_time': 0.075352286, 'prompt_time': 0.015965686, 'queue_time': 0.009121942000000001, 'total_time': 0.091317972}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--de625bd1-cddd-4948-970e-ef72642bd17e-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is Computer Vision?'}, 'id': '8e1keryx9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 312, 'output_tokens': 22, 'total_tokens': 334}),\n",
       "  ToolMessage(content=\"off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyIntroductionOn this pageIntroduction\\n\\nagents.AgentAction\\nRepresents a request to execute an action by an agent.\\n\\nagents.AgentActionMessageLog\\nRepresentation of an action to be executed by an agent.\\n\\nagents.AgentFinish\\nFinal return value of an ActionAgent.\\n\\nagents.AgentStep\\nResult of running an AgentAction.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      Â© Copyright 2025, LangChain Inc.\\n\\nDevelopment: Build your applications using LangChain's open-source components and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\\n\\n\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the integrations page for\\nmore.\\n\\nto use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector\", name='retriever_vector_langchain_blog', id='39ddb4ee-760c-4a14-aa20-b9790a462ff1', tool_call_id='8e1keryx9'),\n",
       "  AIMessage(content='The initial question \"What is Computer Vision?\" is quite broad and doesn\\'t provide much context. To formulate an improved question, let\\'s break it down and try to reason about the underlying meaning and semantics.\\n\\nComputer Vision is a field of study that involves enabling computers to interpret and understand visual information from the world. It\\'s an interdisciplinary field that combines computer science, electrical engineering, mathematics, and psychology to develop algorithms and statistical models that allow computers to process, analyze, and understand digital images and videos.\\n\\nWith this understanding, an improved question could be:\\n\\n\"What are the fundamental concepts, techniques, and applications of Computer Vision, and how does it enable computers to interpret and understand visual data from images and videos?\"\\n\\nAlternatively, more specific improved questions could be:\\n\\n* \"How does Computer Vision work, and what are its key applications in areas like image recognition, object detection, and scene understanding?\"\\n* \"What are the differences between traditional Computer Vision approaches and deep learning-based methods, and how have they impacted the field?\"\\n* \"How is Computer Vision used in real-world applications, such as self-driving cars, medical imaging, and surveillance systems?\"\\n\\nThese improved questions demonstrate a clearer understanding of the topic and allow for more in-depth and specific discussions about Computer Vision.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 251, 'prompt_tokens': 77, 'total_tokens': 328, 'completion_time': 0.734505448, 'prompt_time': 0.004101558, 'queue_time': 0.008745753, 'total_time': 0.738607006}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--85955fcc-c3e0-47ce-b87e-e658ffa66aa1-0', usage_metadata={'input_tokens': 77, 'output_tokens': 251, 'total_tokens': 328}),\n",
       "  AIMessage(content=' \\n\\n', additional_kwargs={'tool_calls': [{'id': 'b5sbyzweh', 'function': {'arguments': '{\"query\":\"What are the fundamental concepts, techniques, and applications of Computer Vision?\"}', 'name': 'retriever_vector_langGraph_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 1007, 'total_tokens': 1039, 'completion_time': 0.066969529, 'prompt_time': 0.063622301, 'queue_time': 0.009468269999999987, 'total_time': 0.13059183}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--43dda3fb-a5c0-4270-a7ed-92871e53bce3-0', tool_calls=[{'name': 'retriever_vector_langGraph_blog', 'args': {'query': 'What are the fundamental concepts, techniques, and applications of Computer Vision?'}, 'id': 'b5sbyzweh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1007, 'output_tokens': 32, 'total_tokens': 1039}),\n",
       "  ToolMessage(content='2. Extraction of structured information from the conversation history\\u200b\\nPlease see long-term memory agent tutorial implements an agent that can extract structured information from the conversation history.\\nMemory classes that fall into this category include:\\nMemory TypeDescriptionBaseEntityStoreAn abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as a dictionary of key-value pairs.ConversationEntityMemoryCombines the ability to summarize the conversation while extracting structured information from the conversation history.\\nAnd specific backend implementations of abstractions:\\n\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I\\'m an AI assistant who can help you search for information using specialized search tools. Is there anything specific you\\'d like to know about or search for? I\\'m happy to help you find accurate and up-to-date information on various topics.\\ninput_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it.\\n\\nLangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€Garrett SpongPrincipal SWE â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€Andres TorresSr. Solutions Architectâ€œIt\\'s easy to build the prototype of a coding agent, but deceptively hard to improve its reliability. Replit wants to\\n\\nto Migration GuideProvides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.VectorStoreRetrieverMemorySee related long-term memory agent tutorialStores the conversation history in a vector store and retrieves the most relevant parts of past conversation based on the input.', name='retriever_vector_langGraph_blog', id='6850b152-002b-428f-8d1c-76f8452fedf9', tool_call_id='b5sbyzweh'),\n",
       "  AIMessage(content='To better understand and address the query, let\\'s break down the components and implications of the initial question, \"What is Computer Vision?\" and then formulate an improved version that could elicit more specific, detailed, or relevant information based on the context of computer science and technology.\\n\\n### Analysis of the Initial Question\\n- **Broad Scope**: The question is quite broad, encompassing a wide range of topics within the field of computer science.\\n- **Lack of Specificity**: It does not specify what aspect of computer vision the person is interested in (e.g., applications, algorithms, history).\\n- **Context**: Without additional context, it\\'s challenging to tailor the response to the questioner\\'s level of knowledge or interest.\\n\\n### Formulating the Improved Question\\nGiven the broad nature of the initial question, an improved question could aim to narrow down the scope or clarify the specific area of interest within computer vision. Here are a few examples:\\n\\n1. **For Beginners**: \"What are the fundamental principles and applications of computer vision in modern technology?\"\\n   - This version seeks to understand both the basics and how computer vision is used in current applications.\\n\\n2. **For Those Interested in Technical Details**: \"What are the key algorithms and techniques used in computer vision for image and video analysis?\"\\n   - This question dives deeper into the technical aspects, focusing on the methodologies and tools used in the field.\\n\\n3. **For Those Interested in Applications**: \"How is computer vision being applied in real-world scenarios, such as healthcare, autonomous vehicles, and security?\"\\n   - This version emphasizes the practical uses and impact of computer vision in various industries.\\n\\n4. **For Those Looking for Future Developments**: \"What are the current challenges and future directions in the field of computer vision, including potential advancements and innovations?\"\\n   - This question looks towards the future, seeking information on ongoing research, challenges, and potential breakthroughs.\\n\\nEach of these improved questions seeks to provide a clearer direction for the response, catering to different levels of interest and knowledge in computer vision.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 410, 'prompt_tokens': 77, 'total_tokens': 487, 'completion_time': 1.332835014, 'prompt_time': 0.010548731, 'queue_time': 0.0088422, 'total_time': 1.3433837450000001}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a2da8abd-c32e-4720-a3c5-6bacb1c8b816-0', usage_metadata={'input_tokens': 77, 'output_tokens': 410, 'total_tokens': 487}),\n",
       "  AIMessage(content=\" \\n\\nLet's ask the improved question: \\n\\n\", additional_kwargs={'tool_calls': [{'id': '53mt4ttkh', 'function': {'arguments': '{\"query\":\"What are the fundamental principles and applications of computer vision in modern technology?\"}', 'name': 'retriever_vector_langGraph_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 2033, 'total_tokens': 2073, 'completion_time': 0.111268637, 'prompt_time': 0.129035173, 'queue_time': 0.010444662999999993, 'total_time': 0.24030381}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f0db3f4f-59ee-4b18-be1f-3caa61db87e6-0', tool_calls=[{'name': 'retriever_vector_langGraph_blog', 'args': {'query': 'What are the fundamental principles and applications of computer vision in modern technology?'}, 'id': '53mt4ttkh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2033, 'output_tokens': 40, 'total_tokens': 2073}),\n",
       "  ToolMessage(content=\"2. Extraction of structured information from the conversation history\\u200b\\nPlease see long-term memory agent tutorial implements an agent that can extract structured information from the conversation history.\\nMemory classes that fall into this category include:\\nMemory TypeDescriptionBaseEntityStoreAn abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as a dictionary of key-value pairs.ConversationEntityMemoryCombines the ability to summarize the conversation while extracting structured information from the conversation history.\\nAnd specific backend implementations of abstractions:\\n\\nLangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€Garrett SpongPrincipal SWE â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€Andres TorresSr. Solutions Architectâ€œIt's easy to build the prototype of a coding agent, but deceptively hard to improve its reliability. Replit wants to\\n\\nâ€œLangChain is streets ahead with what they've put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€Garrett SpongPrincipal SWE â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€Andres TorresSr. Solutions Architectâ€œIt's easy to build the prototype of a coding\\n\\nIntegrated developer experienceSimplify prototyping, debugging, and sharing of agents in our visual LangGraph Studio. Deploy your application with 1-click deploy with our SaaS offering or within your own VPC. Then, monitor app performance with LangSmith.Without LangGraph PlatformWrite your own API endpoints for human-in-the-loop, background jobs, and more. Manage state and checkpointing. \\u2028Handle horizontal scaling and engineer fault tolerance. Continual maintenance and on-call.With LangGraph PlatformFocus on the app logic, not the infrastructure. Full batteries included â€” APIs, scalability, streaming, built in.Developers trust LangGraph to build reliable agents.LangGraph helps teams of all sizes, across all industries, build reliable agents ready for production.Hear how industry leaders use LangGraph\", name='retriever_vector_langGraph_blog', id='ef9891d0-07a6-491f-a9ca-9bfd02e1b792', tool_call_id='53mt4ttkh'),\n",
       "  AIMessage(content='To better understand the underlying meaning and semantics of the initial question, \"What is Computer Vision?\", let\\'s break it down:\\n\\n1. **Subject**: The subject of the question is \"Computer Vision.\"\\n2. **Nature of Inquiry**: The question is seeking a definition or an explanation of what Computer Vision entails.\\n3. **Context**: The context implies a desire to understand the concept, its applications, and possibly its significance in the field of computer science or technology.\\n\\nGiven this analysis, an Improved Question could be formulated to make the inquiry more specific, detailed, and potentially more informative. Here are a few options:\\n\\n- **Option 1**: What are the fundamental principles, applications, and current advancements in Computer Vision?\\n- **Option 2**: How does Computer Vision work, and what are its key applications in industries such as healthcare, security, and automotive?\\n- **Option 3**: Can you provide a comprehensive overview of Computer Vision, including its history, techniques, and future prospects?\\n\\nAmong these, **Option 1** seems to encapsulate a broad and detailed inquiry that could lead to a comprehensive understanding of Computer Vision, covering its basics, its role in various applications, and the latest developments in the field.\\n\\n**Improved Question**: What are the fundamental principles, applications, and current advancements in Computer Vision?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 265, 'prompt_tokens': 77, 'total_tokens': 342, 'completion_time': 0.817286513, 'prompt_time': 0.003642225, 'queue_time': 0.021550697, 'total_time': 0.820928738}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c4b12a10-099c-405a-b22d-11033372e726-0', usage_metadata={'input_tokens': 77, 'output_tokens': 265, 'total_tokens': 342}),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 3006, 'total_tokens': 3007, 'completion_time': 0.007005614, 'prompt_time': 0.179835123, 'queue_time': -9223372037.03461, 'total_time': 0.186840737}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--aee9df55-4ad0-4f6c-af6d-9f59505ea136-0', usage_metadata={'input_tokens': 3006, 'output_tokens': 1, 'total_tokens': 3007})]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Computer Vision?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3c9e2e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________ CALL AGENT ___________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\AI_AGENTS\\Python_N_Env\\LangGraph\\graphenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "---TRANSFORM QUERY---\n",
      "__________________ CALL AGENT ___________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Machine learning?', additional_kwargs={}, response_metadata={}, id='442700cf-07b2-449c-9a76-a2b0e8e18b2f'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '6hnmx7f3y', 'function': {'arguments': '{\"query\":\"Machine learning definition\"}', 'name': 'retriever_vector_langGraph_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 312, 'total_tokens': 333, 'completion_time': 0.062221044, 'prompt_time': 0.023627134, 'queue_time': 0.008770452999999998, 'total_time': 0.085848178}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--2589dde5-ee8a-4c56-84f6-9bf3165956f6-0', tool_calls=[{'name': 'retriever_vector_langGraph_blog', 'args': {'query': 'Machine learning definition'}, 'id': '6hnmx7f3y', 'type': 'tool_call'}], usage_metadata={'input_tokens': 312, 'output_tokens': 21, 'total_tokens': 333}),\n",
       "  ToolMessage(content=\"â€œLangChain is streets ahead with what they've put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€Garrett SpongPrincipal SWE â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€Andres TorresSr. Solutions Architectâ€œIt's easy to build the prototype of a coding\\n\\nLangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€Garrett SpongPrincipal SWE â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€Andres TorresSr. Solutions Architectâ€œIt's easy to build the prototype of a coding agent, but deceptively hard to improve its reliability. Replit wants to\\n\\nthe prototype of a coding agent, but deceptively hard to improve its reliability. Replit wants to give a coding agent to millions of users â€” reliability is our top priority, and will remain so for a long time. LangGraph is giving us the control and ergonomics we need to build and ship powerful coding agents.â€â€œAs Ally advances its exploration of Generative AI, Michele CatastaPresidentâ€œAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.â€â€œAs Ally advances its exploration of Generative AI, Sathish MuthukrishnanChief Information, Data and Digital Officer\\n\\nEvolution of memory in LangChain\\u200b\\nThe concept of memory has evolved significantly in LangChain since its initial release.\\nLangChain 0.0.x memory\\u200b\\nBroadly speaking, LangChain 0.0.x memory was used to handle three main use cases:\\nUse CaseExampleManaging conversation historyKeep only the last n turns of the conversation between the user and the AI.Extraction of structured informationExtract structured information from the conversation history, such as a list of facts learned about the user.Composite memory implementationsCombine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation.\\nWhile the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.\", name='retriever_vector_langGraph_blog', id='8fcdc6d7-fa10-4b7c-b146-e5ebc991cc6b', tool_call_id='6hnmx7f3y'),\n",
       "  AIMessage(content='To formulate an improved question, let\\'s break down the initial question and analyze its components:\\n\\n1. **Subject**: The subject of the question is \"Machine learning.\"\\n2. **Nature of the Question**: The question is asking for a definition or explanation of what machine learning is.\\n3. **Information Sought**: The person asking the question is likely seeking a basic understanding or an overview of the concept of machine learning.\\n\\nGiven this analysis, an improved question could be more specific and detailed to elicit a more comprehensive and tailored response. Here are a few possibilities:\\n\\n- **For a Beginner**: \"What is machine learning, and how does it differ from traditional programming?\"\\n- **For a Technical Audience**: \"Can you explain the fundamental principles and types of machine learning, including supervised, unsupervised, and reinforcement learning?\"\\n- **For an Application-Focused Inquiry**: \"How does machine learning apply to real-world problems, and what are some of its most impactful applications?\"\\n- **For Educational Purposes**: \"What are the key concepts and techniques in machine learning that a beginner should understand to get started in the field?\"\\n\\nAmong these, a well-rounded improved question that could cater to a broad range of interests and knowledge levels might be:\\n\\n**Improved Question**: \"What are the core principles of machine learning, and how is it applied across different industries and domains?\"\\n\\nThis revised question seeks not only a definition but also an understanding of the applications and relevance of machine learning, potentially leading to a more informative and engaging response.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 305, 'prompt_tokens': 77, 'total_tokens': 382, 'completion_time': 1.06978581, 'prompt_time': 0.003848373, 'queue_time': 0.008671603, 'total_time': 1.073634183}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3e6327e3-6879-436b-ba34-96f51b08207b-0', usage_metadata={'input_tokens': 77, 'output_tokens': 305, 'total_tokens': 382}),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 1391, 'total_tokens': 1392, 'completion_time': 0.006938505, 'prompt_time': 0.084364374, 'queue_time': -9223372036.93914, 'total_time': 0.091302879}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7975e170-e049-4b2e-86c7-0e217ce93d22-0', usage_metadata={'input_tokens': 1391, 'output_tokens': 1, 'total_tokens': 1392})]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2aff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
